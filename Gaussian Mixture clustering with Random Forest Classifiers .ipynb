{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here are some imports that are used along this notebook\n",
    "import math\n",
    "import itertools\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline\n",
    "gt0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "# Creating local SparkContext with 8 threads and SQLContext based on it\n",
    "sc = pyspark.SparkContext(master='local[8]')\n",
    "sc.setLogLevel('INFO')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading train data\n",
    "t0 = time()\n",
    "train_df = load_dataset(train_nsl_kdd_dataset_path)\n",
    "\n",
    "# Fitting preparation pipeline\n",
    "labels_mapping_model = labels_mapping_pipeline.fit(train_df)\n",
    "\n",
    "# Transforming labels column and adding id column\n",
    "train_df = labels_mapping_model.transform(train_df).withColumn('id', sql.monotonically_increasing_id())\n",
    "\n",
    "train_df = train_df.cache()\n",
    "print(train_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "0.778895378112793\n"
     ]
    }
   ],
   "source": [
    "# Loading test data\n",
    "t0 = time()\n",
    "test_df = load_dataset(test_nsl_kdd_dataset_path)\n",
    "\n",
    "# Transforming labels column and adding id column\n",
    "test_df = labels_mapping_model.transform(test_df).withColumn('id', sql.monotonically_increasing_id())\n",
    "\n",
    "test_df = test_df.cache()\n",
    "print(test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One Hot Encoding for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding (OHE) is used for treating categorical variables. Custom function is created for demonstration purposes. However, it could be easily replaced by PySpark OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ohe_vec(cat_dict, row):\n",
    "    vec = np.zeros(len(cat_dict))\n",
    "    vec[cat_dict[row]] = float(1.0)\n",
    "    return vec.tolist()\n",
    "\n",
    "def ohe(df, nominal_col):\n",
    "    categories = (df.select(nominal_col)\n",
    "                    .distinct()\n",
    "                    .rdd.map(lambda row: row[0])\n",
    "                    .collect())\n",
    "    \n",
    "    cat_dict = dict(zip(categories, range(len(categories))))\n",
    "    \n",
    "    udf_ohe_vec = udf(lambda row: ohe_vec(cat_dict, row), \n",
    "                      StructType([StructField(cat, DoubleType(), False) for cat in categories]))\n",
    "    \n",
    "    df = df.withColumn(nominal_col + '_ohe', udf_ohe_vec(col(nominal_col))).cache()\n",
    "    \n",
    "    nested_cols = [nominal_col + '_ohe.' + cat for cat in categories]\n",
    "    ohe_cols = [nominal_col + '_' + cat for cat in categories]\n",
    "        \n",
    "    for new, old in zip(ohe_cols, nested_cols):\n",
    "        df = df.withColumn(new, col(old))\n",
    "\n",
    "    df = df.drop(nominal_col + '_ohe')\n",
    "                   \n",
    "    return df, ohe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "13.566007614135742\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "train_ohe_cols = []\n",
    "\n",
    "train_df, train_ohe_col0 = ohe(train_df, nominal_cols[0])\n",
    "train_ohe_cols += train_ohe_col0\n",
    "\n",
    "train_df, train_ohe_col1 = ohe(train_df, nominal_cols[1])\n",
    "train_ohe_cols += train_ohe_col1\n",
    "\n",
    "train_df, train_ohe_col2 = ohe(train_df, nominal_cols[2])\n",
    "train_ohe_cols += train_ohe_col2\n",
    "\n",
    "binary_cols += train_ohe_cols\n",
    "\n",
    "train_df = train_df.cache()\n",
    "print(train_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom list of test binary cols is used as test dataset could contain additional categories for 'service' and 'flag' features. However, those additional categories aren't used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "9.082245588302612\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "test_ohe_cols = []\n",
    "\n",
    "test_df, test_ohe_col0_names = ohe(test_df, nominal_cols[0])\n",
    "test_ohe_cols += test_ohe_col0_names\n",
    "\n",
    "test_df, test_ohe_col1_names = ohe(test_df, nominal_cols[1])\n",
    "test_ohe_cols += test_ohe_col1_names\n",
    "\n",
    "test_df, test_ohe_col2_names = ohe(test_df, nominal_cols[2])\n",
    "test_ohe_cols += test_ohe_col2_names\n",
    "\n",
    "test_binary_cols = col_names[binary_inx].tolist() + test_ohe_cols\n",
    "\n",
    "test_df = test_df.cache()\n",
    "print(test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=selectFeaturesByAR(ar_dict, 0.01), outputCol='raw_features')\n",
    "indexer = VectorIndexer(inputCol='raw_features', outputCol='indexed_features', maxCategories=2)\n",
    "\n",
    "prep_pipeline = Pipeline(stages=[assembler, indexer])\n",
    "prep_model = prep_pipeline.fit(scaled_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "22544\n",
      "1.659245252609253\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "scaled_train_df = (prep_model\n",
    "        .transform(scaled_train_df)\n",
    "        .select('id', 'indexed_features', 'labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
    "        .cache())\n",
    "\n",
    "scaled_test_df = (prep_model \n",
    "        .transform(scaled_test_df)\n",
    "        .select('id', 'indexed_features','labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
    "        .cache())\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100840\n",
      "25133\n"
     ]
    }
   ],
   "source": [
    "split = (scaled_train_df.randomSplit([0.8, 0.2], seed=seed))\n",
    "\n",
    "scaled_train_df = split[0].cache()\n",
    "scaled_cv_df = split[1].cache()\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_cv_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25133\n",
      "22544\n"
     ]
    }
   ],
   "source": [
    "res_cv_df = scaled_cv_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
    "res_test_df = scaled_test_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
    "prob_cols = []\n",
    "pred_cols = []\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different metrics from sklearn are used for evaluating results. The most important from them for this task are False positive Rate, Detection Rate and F1 score. \n",
    "As evaluating via sklearn requires to collect predicted and label columns to the driver, it will be replaced with PySpark metrics later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def printCM(cm, labels):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels])\n",
    "    # Print header\n",
    "    print(\" \" * columnwidth, end=\"\\t\")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\"\\t\")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"%{0}s\".format(columnwidth) % label1, end=\"\\t\")\n",
    "        for j in range(len(labels)):\n",
    "            print(\"%{0}d\".format(columnwidth) % cm[i, j], end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "def getPrediction(e):\n",
    "    return udf(lambda row: 1.0 if row >= e else 0.0, DoubleType())\n",
    "        \n",
    "def printReport(resDF, probCol, labelCol='labels2_index', e=None, labels=['normal', 'attack']):\n",
    "    if (e):\n",
    "        predictionAndLabels = list(zip(*resDF.rdd\n",
    "                                       .map(lambda row: (1.0 if row[probCol] >= e else 0.0, row[labelCol]))\n",
    "                                       .collect()))\n",
    "    else:\n",
    "        predictionAndLabels = list(zip(*resDF.rdd\n",
    "                                       .map(lambda row: (row[probCol], row[labelCol]))\n",
    "                                       .collect()))\n",
    "    \n",
    "    cm = metrics.confusion_matrix(predictionAndLabels[1], predictionAndLabels[0])\n",
    "    printCM(cm, labels)\n",
    "    print(\" \")\n",
    "    print(\"Accuracy = %g\" % (metrics.accuracy_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
    "    print(\"AUC = %g\" % (metrics.roc_auc_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
    "    print(\" \")\n",
    "    print(\"False Alarm Rate = %g\" % (cm[0][1]/(cm[0][0] + cm[0][1])))\n",
    "    print(\"Detection Rate = %g\" % (cm[1][1]/(cm[1][1] + cm[1][0])))\n",
    "    print(\"F1 score = %g\" % (metrics.f1_score(predictionAndLabels[1], predictionAndLabels[0], labels)))\n",
    "    print(\" \")\n",
    "    print(metrics.classification_report(predictionAndLabels[1], predictionAndLabels[0]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gaussian Mixture clustering with Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gm_prob_col = 'gm_rf_prob'\n",
    "gm_pred_col = 'gm_rf_pred'\n",
    "\n",
    "prob_cols.append(gm_prob_col)\n",
    "pred_cols.append(gm_pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100840\n",
      "25133\n",
      "22544\n",
      "12.079548120498657\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Mixture clustering\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "t0 = time()\n",
    "gm = GaussianMixture(k=8, maxIter=150, seed=seed, featuresCol=\"pca_features\", \n",
    "                     predictionCol=\"cluster\", probabilityCol=\"gm_prob\")\n",
    "\n",
    "gm_pipeline = Pipeline(stages=[pca_slicer, pca, gm])\n",
    "gm_model = gm_pipeline.fit(scaled_train_df)\n",
    "\n",
    "gm_train_df = gm_model.transform(scaled_train_df).cache()\n",
    "gm_cv_df = gm_model.transform(scaled_cv_df).cache()\n",
    "gm_test_df = gm_model.transform(scaled_test_df).cache()\n",
    "\n",
    "gm_params = (gm_model.stages[2].gaussiansDF.rdd\n",
    "                  .map(lambda row: [row['mean'].toArray(), row['cov'].toArray()])\n",
    "                  .collect())\n",
    "gm_weights = gm_model.stages[2].weights\n",
    "\n",
    "print(gm_train_df.count())\n",
    "print(gm_cv_df.count())\n",
    "print(gm_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+------+-----+\n",
      "|cluster_labels2|attack|normal|count|\n",
      "+---------------+------+------+-----+\n",
      "|              0| 22895|     0|22895|\n",
      "|              1|  7548| 19846|27394|\n",
      "|              2|    30| 26900|26930|\n",
      "|              3|  4024|     0| 4024|\n",
      "|              4|  4154|  2212| 6366|\n",
      "|              5|  1136|  1007| 2143|\n",
      "|              6|  5479|     0| 5479|\n",
      "|              7|  1559|  4050| 5609|\n",
      "+---------------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Description of the contents of the clusters \n",
    "gm_crosstab = getClusterCrosstab(gm_train_df).cache()\n",
    "gm_crosstab.show(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n",
      "{0: 1.0, 3: 1.0, 6: 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: [27394, 0.27553478863984815],\n",
       " 2: [26930, 0.0011139992573338284],\n",
       " 4: [6366, 0.6525290606346215],\n",
       " 5: [2143, 0.5300979934671022],\n",
       " 7: [5609, 0.27794615796042077]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting clusters\n",
    "gm_cluster_rf, gm_cluster_mapping = splitClusters(gm_crosstab)\n",
    "\n",
    "print(len(gm_cluster_rf), len(gm_cluster_mapping))\n",
    "print(gm_cluster_mapping)\n",
    "gm_cluster_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 cluster in 89.9797 ms\n",
      "Finished 2 cluster in 5.05247 ms\n",
      "Finished 4 cluster in 12.6685 ms\n",
      "Finished 5 cluster in 12.9819 ms\n",
      "Finished 7 cluster in 24.1031 ms\n",
      "144.7893054485321\n"
     ]
    }
   ],
   "source": [
    "# Training Random Forest classifiers for each of the clusters\n",
    "t0 = time()\n",
    "gm_cluster_models = getClusterModels(gm_train_df, gm_cluster_rf)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25133\n",
      "22.402843713760376\n"
     ]
    }
   ],
   "source": [
    "# Getting probabilities for CV data\n",
    "t0 = time()\n",
    "res_cv_df = (res_cv_df.drop(gm_prob_col)\n",
    "             .join(getProbabilities(gm_cv_df, gm_prob_col, gm_cluster_mapping, gm_cluster_models), 'id')\n",
    "             .cache())\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "23.39514684677124\n"
     ]
    }
   ],
   "source": [
    "# Getting probabilities for Test data\n",
    "t0 = time()\n",
    "res_test_df = (res_test_df.drop(gm_prob_col)\n",
    "               .join(getProbabilities(gm_test_df, gm_prob_col, gm_cluster_mapping, gm_cluster_models), 'id')\n",
    "               .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \tnormal\tattack\t\n",
      "normal\t 13322\t     6\t\n",
      "attack\t    36\t 11769\t\n",
      " \n",
      "Accuracy = 0.998329\n",
      "AUC = 0.99825\n",
      " \n",
      "False Alarm Rate = 0.00045018\n",
      "Detection Rate = 0.99695\n",
      "F1 score = 0.998219\n",
      " \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00     13328\n",
      "        1.0       1.00      1.00      1.00     11805\n",
      "\n",
      "avg / total       1.00      1.00      1.00     25133\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "printReport(res_cv_df, gm_prob_col, e=0.5, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \tnormal\tattack\t\n",
      "normal\t  8340\t  1371\t\n",
      "attack\t   727\t 12106\t\n",
      " \n",
      "Accuracy = 0.906938\n",
      "AUC = 0.901085\n",
      " \n",
      "False Alarm Rate = 0.14118\n",
      "Detection Rate = 0.943349\n",
      "F1 score = 0.920258\n",
      " \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.86      0.89      9711\n",
      "        1.0       0.90      0.94      0.92     12833\n",
      "\n",
      "avg / total       0.91      0.91      0.91     22544\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "printReport(res_test_df, gm_prob_col, e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25133\n",
      "22544\n",
      "17.49277424812317\n"
     ]
    }
   ],
   "source": [
    "# Adding prediction columns based on chosen thresholds into result dataframes\n",
    "t0 = time()\n",
    "res_cv_df = res_cv_df.withColumn(gm_pred_col, getPrediction(0.5)(col(gm_prob_col))).cache()\n",
    "res_test_df = res_test_df.withColumn(gm_pred_col, getPrediction(0.01)(col(gm_prob_col))).cache()\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
