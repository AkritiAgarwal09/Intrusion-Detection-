{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here are some imports that are used along this notebook\n",
    "import math\n",
    "import itertools\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline\n",
    "gt0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "# Creating local SparkContext with 8 threads and SQLContext based on it\n",
    "sc = pyspark.SparkContext(master='local[8]')\n",
    "sc.setLogLevel('INFO')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading train data\n",
    "t0 = time()\n",
    "train_df = load_dataset(train_nsl_kdd_dataset_path)\n",
    "\n",
    "# Fitting preparation pipeline\n",
    "labels_mapping_model = labels_mapping_pipeline.fit(train_df)\n",
    "\n",
    "# Transforming labels column and adding id column\n",
    "train_df = labels_mapping_model.transform(train_df).withColumn('id', sql.monotonically_increasing_id())\n",
    "\n",
    "train_df = train_df.cache()\n",
    "print(train_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "0.778895378112793\n"
     ]
    }
   ],
   "source": [
    "# Loading test data\n",
    "t0 = time()\n",
    "test_df = load_dataset(test_nsl_kdd_dataset_path)\n",
    "\n",
    "# Transforming labels column and adding id column\n",
    "test_df = labels_mapping_model.transform(test_df).withColumn('id', sql.monotonically_increasing_id())\n",
    "\n",
    "test_df = test_df.cache()\n",
    "print(test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ohe_vec(cat_dict, row):\n",
    "    vec = np.zeros(len(cat_dict))\n",
    "    vec[cat_dict[row]] = float(1.0)\n",
    "    return vec.tolist()\n",
    "\n",
    "def ohe(df, nominal_col):\n",
    "    categories = (df.select(nominal_col)\n",
    "                    .distinct()\n",
    "                    .rdd.map(lambda row: row[0])\n",
    "                    .collect())\n",
    "    \n",
    "    cat_dict = dict(zip(categories, range(len(categories))))\n",
    "    \n",
    "    udf_ohe_vec = udf(lambda row: ohe_vec(cat_dict, row), \n",
    "                      StructType([StructField(cat, DoubleType(), False) for cat in categories]))\n",
    "    \n",
    "    df = df.withColumn(nominal_col + '_ohe', udf_ohe_vec(col(nominal_col))).cache()\n",
    "    \n",
    "    nested_cols = [nominal_col + '_ohe.' + cat for cat in categories]\n",
    "    ohe_cols = [nominal_col + '_' + cat for cat in categories]\n",
    "        \n",
    "    for new, old in zip(ohe_cols, nested_cols):\n",
    "        df = df.withColumn(new, col(old))\n",
    "\n",
    "    df = df.drop(nominal_col + '_ohe')\n",
    "                   \n",
    "    return df, ohe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "13.566007614135742\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "train_ohe_cols = []\n",
    "\n",
    "train_df, train_ohe_col0 = ohe(train_df, nominal_cols[0])\n",
    "train_ohe_cols += train_ohe_col0\n",
    "\n",
    "train_df, train_ohe_col1 = ohe(train_df, nominal_cols[1])\n",
    "train_ohe_cols += train_ohe_col1\n",
    "\n",
    "train_df, train_ohe_col2 = ohe(train_df, nominal_cols[2])\n",
    "train_ohe_cols += train_ohe_col2\n",
    "\n",
    "binary_cols += train_ohe_cols\n",
    "\n",
    "train_df = train_df.cache()\n",
    "print(train_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom list of test binary cols is used as test dataset could contain additional categories for 'service' and 'flag' features. However, those additional categories aren't used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "9.082245588302612\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "test_ohe_cols = []\n",
    "\n",
    "test_df, test_ohe_col0_names = ohe(test_df, nominal_cols[0])\n",
    "test_ohe_cols += test_ohe_col0_names\n",
    "\n",
    "test_df, test_ohe_col1_names = ohe(test_df, nominal_cols[1])\n",
    "test_ohe_cols += test_ohe_col1_names\n",
    "\n",
    "test_df, test_ohe_col2_names = ohe(test_df, nominal_cols[2])\n",
    "test_ohe_cols += test_ohe_col2_names\n",
    "\n",
    "test_binary_cols = col_names[binary_inx].tolist() + test_ohe_cols\n",
    "\n",
    "test_df = test_df.cache()\n",
    "print(test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=selectFeaturesByAR(ar_dict, 0.01), outputCol='raw_features')\n",
    "indexer = VectorIndexer(inputCol='raw_features', outputCol='indexed_features', maxCategories=2)\n",
    "\n",
    "prep_pipeline = Pipeline(stages=[assembler, indexer])\n",
    "prep_model = prep_pipeline.fit(scaled_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125973\n",
      "22544\n",
      "1.659245252609253\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "scaled_train_df = (prep_model\n",
    "        .transform(scaled_train_df)\n",
    "        .select('id', 'indexed_features', 'labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
    "        .cache())\n",
    "\n",
    "scaled_test_df = (prep_model \n",
    "        .transform(scaled_test_df)\n",
    "        .select('id', 'indexed_features','labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
    "        .cache())\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100840\n",
      "25133\n"
     ]
    }
   ],
   "source": [
    "split = (scaled_train_df.randomSplit([0.8, 0.2], seed=seed))\n",
    "\n",
    "scaled_train_df = split[0].cache()\n",
    "scaled_cv_df = split[1].cache()\n",
    "\n",
    "print(scaled_train_df.count())\n",
    "print(scaled_cv_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25133\n",
      "22544\n"
     ]
    }
   ],
   "source": [
    "res_cv_df = scaled_cv_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
    "res_test_df = scaled_test_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
    "prob_cols = []\n",
    "pred_cols = []\n",
    "\n",
    "print(res_cv_df.count())\n",
    "print(res_test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different metrics from sklearn are used for evaluating results. The most important from them for this task are False positive Rate, Detection Rate and F1 score. \n",
    "As evaluating via sklearn requires to collect predicted and label columns to the driver, it will be replaced with PySpark metrics later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def printCM(cm, labels):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels])\n",
    "    # Print header\n",
    "    print(\" \" * columnwidth, end=\"\\t\")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\"\\t\")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"%{0}s\".format(columnwidth) % label1, end=\"\\t\")\n",
    "        for j in range(len(labels)):\n",
    "            print(\"%{0}d\".format(columnwidth) % cm[i, j], end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "def getPrediction(e):\n",
    "    return udf(lambda row: 1.0 if row >= e else 0.0, DoubleType())\n",
    "        \n",
    "def printReport(resDF, probCol, labelCol='labels2_index', e=None, labels=['normal', 'attack']):\n",
    "    if (e):\n",
    "        predictionAndLabels = list(zip(*resDF.rdd\n",
    "                                       .map(lambda row: (1.0 if row[probCol] >= e else 0.0, row[labelCol]))\n",
    "                                       .collect()))\n",
    "    else:\n",
    "        predictionAndLabels = list(zip(*resDF.rdd\n",
    "                                       .map(lambda row: (row[probCol], row[labelCol]))\n",
    "                                       .collect()))\n",
    "    \n",
    "    cm = metrics.confusion_matrix(predictionAndLabels[1], predictionAndLabels[0])\n",
    "    printCM(cm, labels)\n",
    "    print(\" \")\n",
    "    print(\"Accuracy = %g\" % (metrics.accuracy_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
    "    print(\"AUC = %g\" % (metrics.roc_auc_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
    "    print(\" \")\n",
    "    print(\"False Alarm Rate = %g\" % (cm[0][1]/(cm[0][0] + cm[0][1])))\n",
    "    print(\"Detection Rate = %g\" % (cm[1][1]/(cm[1][1] + cm[1][0])))\n",
    "    print(\"F1 score = %g\" % (metrics.f1_score(predictionAndLabels[1], predictionAndLabels[0], labels)))\n",
    "    print(\" \")\n",
    "    print(metrics.classification_report(predictionAndLabels[1], predictionAndLabels[0]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ensembling experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Logistic Regression and Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "32.770461082458496\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "t0 = time()\n",
    "lr_assembler = VectorAssembler(inputCols=[\n",
    "                            kmeans_prob_col, \n",
    "                            gm_prob_col, \n",
    "                            dos_prob_col, \n",
    "                            probe_prob_col, \n",
    "                            r2l_u2r_prob_col\n",
    "                            ], \n",
    "                            outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=100, labelCol=\"labels2_index\", standardization=False, weightCol='weights')\n",
    "lr_pipeline = Pipeline(stages=[lr_assembler, lr])\n",
    "\n",
    "weights_dict = {\n",
    "    'normal': 1.0,\n",
    "    'DoS': 100.0,\n",
    "    'Probe': 100.0,\n",
    "    'R2L': 100.0,\n",
    "    'U2R': 100.0\n",
    "}\n",
    "\n",
    "udf_weight = udf(lambda row: weights_dict[row], DoubleType())\n",
    "lr_model = lr_pipeline.fit(res_cv_df.withColumn('weights', udf_weight('labels5')))\n",
    "lr_test_df = lr_model.transform(res_test_df).cache()\n",
    "\n",
    "res_test_df = (res_test_df.drop('lr_prob')\n",
    "                    .join(lr_test_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', 'lr_prob']), 'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \tnormal\tattack\t\n",
      "normal\t  8166\t  1545\t\n",
      "attack\t   112\t 12721\t\n",
      " \n",
      "Accuracy = 0.926499\n",
      "AUC = 0.916087\n",
      " \n",
      "False Alarm Rate = 0.159098\n",
      "Detection Rate = 0.991273\n",
      "F1 score = 0.938854\n",
      " \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.84      0.91      9711\n",
      "        1.0       0.89      0.99      0.94     12833\n",
      "\n",
      "avg / total       0.93      0.93      0.93     22544\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "printReport(res_test_df, 'lr_prob', e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "28.341699600219727\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "rf_assembler = VectorAssembler(inputCols=[\n",
    "                            kmeans_pred_col, \n",
    "                            gm_pred_col, \n",
    "                            dos_pred_col, \n",
    "                            probe_pred_col, \n",
    "                            r2l_u2r_pred_col\n",
    "                            ],\n",
    "                            outputCol='features')\n",
    "\n",
    "rf_indexer =  VectorIndexer(inputCol='features', outputCol='indexed_features', maxCategories=2)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='labels2_index', featuresCol='features', seed=seed,\n",
    "                            numTrees=250, maxDepth=5, featureSubsetStrategy='auto')\n",
    "rf_pipeline = Pipeline(stages=[rf_assembler, \n",
    "                               rf_indexer,\n",
    "                               rf])\n",
    "rf_model = rf_pipeline.fit(res_cv_df)\n",
    "rf_test_df = rf_model.transform(res_test_df).cache()\n",
    "\n",
    "res_test_df = (res_test_df.drop('rf_prob')\n",
    "                    .join(rf_test_df.rdd\n",
    "                    .map(lambda row: (row['id'], float(row['probability'][1])))\n",
    "                    .toDF(['id', 'rf_prob']), 'id')\n",
    "                    .cache())\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \tnormal\tattack\t\n",
      "normal\t  8146\t  1565\t\n",
      "attack\t    88\t 12745\t\n",
      " \n",
      "Accuracy = 0.926677\n",
      "AUC = 0.915993\n",
      " \n",
      "False Alarm Rate = 0.161157\n",
      "Detection Rate = 0.993143\n",
      "F1 score = 0.9391\n",
      " \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.84      0.91      9711\n",
      "        1.0       0.89      0.99      0.94     12833\n",
      "\n",
      "avg / total       0.93      0.93      0.93     22544\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "printReport(res_test_df, 'rf_prob', e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22544\n",
      "4.118284225463867\n"
     ]
    }
   ],
   "source": [
    "# Adding prediction columns based on chosen thresholds into result dataframes\n",
    "t0 = time()\n",
    "res_test_df = res_test_df.withColumn('lr_pred', getPrediction(0.01)(col('lr_prob'))).cache()\n",
    "res_test_df = res_test_df.withColumn('rf_pred', getPrediction(0.01)(col('rf_prob'))).cache()\n",
    "\n",
    "print(res_test_df.count())\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \tnormal\tattack\t\n",
      "normal\t  8146\t  1565\t\n",
      "attack\t    88\t 12745\t\n",
      " \n",
      "Accuracy = 0.926677\n",
      "AUC = 0.915993\n",
      " \n",
      "False Alarm Rate = 0.161157\n",
      "Detection Rate = 0.993143\n",
      "F1 score = 0.9391\n",
      " \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.84      0.91      9711\n",
      "        1.0       0.89      0.99      0.94     12833\n",
      "\n",
      "avg / total       0.93      0.93      0.93     22544\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "printReport(res_test_df\n",
    "            .select('labels2_index', ((col('lr_prob') + col('rf_prob'))/2)\n",
    "                    .alias('voting')), \n",
    "                    'voting', e=0.01, labels=labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \tnormal\tattack\t\n",
      "normal\t  8094\t  1617\t\n",
      "attack\t    47\t 12786\t\n",
      " \n",
      "Accuracy = 0.926189\n",
      "AUC = 0.914913\n",
      " \n",
      "False Alarm Rate = 0.166512\n",
      "Detection Rate = 0.996338\n",
      "F1 score = 0.938904\n",
      " \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.83      0.91      9711\n",
      "        1.0       0.89      1.00      0.94     12833\n",
      "\n",
      "avg / total       0.93      0.93      0.93     22544\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "printReport(res_test_df\n",
    "            .select('labels2_index', (col('lr_pred').cast('int').bitwiseOR(col('rf_pred').cast('int')))\n",
    "                    .alias('voting')), \n",
    "                    'voting', labels=labels2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
